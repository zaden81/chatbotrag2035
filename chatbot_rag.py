# --- imports ---
import streamlit as st
st.set_page_config(page_title="Chatbot Ph√°p lu·∫≠t")  # ph·∫£i l√† l·ªánh Streamlit ƒë·∫ßu ti√™n

import os
from dotenv import load_dotenv
from math import gcd
from scipy.signal import resample_poly

# pinecone
from pinecone import Pinecone

# langchain + pinecone vector store
from langchain_pinecone import PineconeVectorStore

# retrievers & compressors
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.retrievers.multi_query import MultiQueryRetriever

# huggingface & wrappers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline

# reranker (cross-encoder)
from sentence_transformers import CrossEncoder

# mic
from streamlit_mic_recorder import mic_recorder

# audio utils (ƒë·ªÉ ƒë·ªçc WAV t·ª´ bytes, kh√¥ng c·∫ßn ffmpeg)
import io
import numpy as np
import soundfile as sf
from concurrent.futures import ThreadPoolExecutor

import torch
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")

# --- init ---
load_dotenv()
# ƒë·∫£m b·∫£o kh√¥ng ch·∫°y offline HF
os.environ.pop("HF_HUB_OFFLINE", None)
os.environ.pop("TRANSFORMERS_OFFLINE", None)

st.title("Chatbot T∆∞ v·∫•n ph√°p lu·∫≠t")

# Pinecone
pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
index_name = os.environ.get("PINECONE_INDEX_NAME")
index_host = os.environ.get("PINECONE_INDEX_HOST")
index = pc.Index(index_name, host=index_host)  # n·∫øu c·∫ßn host: pc.Index(index_name, host=index_host)

# Embeddings (All-MiniLM-L6-v2 l√† ·ªïn cho t·ªëc/chi ph√≠)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = PineconeVectorStore(index=index, embedding=embeddings)

# Chat history
from langchain_core.messages import HumanMessage, AIMessage
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    role = "user" if isinstance(message, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.markdown(message.content)

# --- LLM & reranker caches ---
@st.cache_resource
def load_gemma():
    model_id = "google/gemma-3-1b-it"
    hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")

    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token, local_files_only=False)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        token=hf_token,
        device_map="cuda" if torch.cuda.is_available() else "cpu",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    # T·∫°o pipeline ƒë·ªÉ d√πng cho MultiQueryRetriever (√≠t random ƒë·ªÉ ·ªïn ƒë·ªãnh c√¢u h·ªèi ph·ª•)
    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=128,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
    )
    llm = HuggingFacePipeline(pipeline=gen_pipe)
    return tokenizer, model, llm

@st.cache_resource
def load_reranker():
    """∆Øu ti√™n model nhanh; n·∫øu l·ªói, fallback sang ƒëa ng√¥n ng·ªØ; n·∫øu v·∫´n l·ªói ‚Üí None."""
    hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")
    try:
        return CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", token=hf_token, max_length=512)
    except Exception as e1:
        st.warning(f"Kh√¥ng t·∫£i ƒë∆∞·ª£c ms-marco-MiniLM-L-6-v2. Th·ª≠ reranker ƒëa ng√¥n ng·ªØ‚Ä¶\n{e1}")
        try:
            return CrossEncoder("jinaai/jina-reranker-v2-base-multilingual", token=hf_token, max_length=512, trust_remote_code=True)
        except Exception as e2:
            st.error(f"Kh√¥ng t·∫£i ƒë∆∞·ª£c reranker (ƒë√£ th·ª≠ 2 model). Ch·∫°y kh√¥ng rerank.\n{e2}")
            return None

# ==== PhoWhisper via transformers (pipeline ASR, kh√¥ng d√πng ffmpeg) ====
@st.cache_resource
def load_asr_pipeline():
    """
    T·∫£i PhoWhisper/Whisper theo th·ª© t·ª± ∆∞u ti√™n.
    Kh√¥ng d√πng use_auth_token/local_files_only trong pipeline.
    ƒê·ªçc WAV t·ª´ bytes b·∫±ng soundfile ‚Üí kh√¥ng c·∫ßn ffmpeg.
    """
    hf_token = os.environ.get("HUGGINGFACE_HUB_TOKEN") or os.environ.get("HUGGINGFACEHUB_API_TOKEN")
    device = 0 if torch.cuda.is_available() else -1
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    repos = [
        "vinai/PhoWhisper-small",  # ∆∞u ti√™n PhoWhisper (TV)
        "openai/whisper-small",    # fallback 1
        "openai/whisper-tiny"      # fallback 2 (nh·∫π ƒë·ªÉ test m·∫°ng/token)
    ]

    last_err = None
    for repo in repos:
        try:
            processor = AutoProcessor.from_pretrained(repo, token=hf_token, local_files_only=False)
            model = AutoModelForSpeechSeq2Seq.from_pretrained(
                repo, token=hf_token, local_files_only=False, torch_dtype=dtype
            )
            asr = pipeline(
                task="automatic-speech-recognition",
                model=model,
                tokenizer=getattr(processor, "tokenizer", processor),
                feature_extractor=getattr(processor, "feature_extractor", processor),
                device=device,
                chunk_length_s=20,
                stride_length_s=(4, 2),
                return_timestamps=False,
            )
            if repo != "vinai/PhoWhisper-small":
                st.info(f"ƒêang d√πng fallback: {repo}")
            return asr
        except Exception as e:
            st.warning(f"Kh√¥ng t·∫£i ƒë∆∞·ª£c {repo}: {e}")
            last_err = e

    st.error(f"ASR ch∆∞a s·∫µn s√†ng. Ki·ªÉm tra HUGGINGFACE_HUB_TOKEN v√† k·∫øt n·ªëi m·∫°ng. Chi ti·∫øt: {last_err}")
    return None

# T·∫£i c√°c t√†i nguy√™n n·∫∑ng song song ƒë·ªÉ gi·∫£m th·ªùi gian ch·ªù
with st.spinner("ƒêang t·∫£i m√¥ h√¨nh v√† t√†i nguy√™n‚Ä¶"):
    with ThreadPoolExecutor(max_workers=3) as executor:
        future_gemma = executor.submit(load_gemma)
        future_reranker = executor.submit(load_reranker)
        future_asr = executor.submit(load_asr_pipeline)
        tokenizer, model, llm = future_gemma.result()
        reranker = future_reranker.result()
        asr_pipe = future_asr.result()

# --- T·∫°o retriever 3 t·∫ßng ---
# 1) Base: MMR ƒë·ªÉ v·ª´a li√™n quan v·ª´a ƒëa d·∫°ng
mmr_retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 12, "fetch_k": 60, "lambda_mult": 0.6},
)

# 2) N√©n/n·ªôi dung: l·ªçc theo ng∆∞·ª°ng similarity ƒë·ªÉ ƒëu·ªïi nhi·ªÖu
compressor = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.62)
compressed_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=mmr_retriever)

# 3) MultiQuery: sinh bi·∫øn th·ªÉ c√¢u h·ªèi tƒÉng recall
mq_retriever = MultiQueryRetriever.from_llm(retriever=compressed_retriever, llm=llm)

# --- Helper: Rerank + guardrail ---
def truncate_for_rerank(text: str, max_chars: int = 2500) -> str:
    return text if len(text) <= max_chars else text[:max_chars]

def retrieve_docs(query: str, final_k: int = 6):
    candidates = mq_retriever.get_relevant_documents(query)

    if not candidates:
        old = compressor.similarity_threshold
        compressor.similarity_threshold = 0.58
        candidates = mq_retriever.get_relevant_documents(query)
        compressor.similarity_threshold = old

    if not candidates:
        simple = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 8})
        candidates = simple.get_relevant_documents(query)

    if not candidates:
        return []

    if reranker is None:
        ranked_docs = candidates[:final_k]
    else:
        pairs = [(query, truncate_for_rerank(d.page_content)) for d in candidates]
        scores = reranker.predict(pairs)
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        seen, ranked_docs = set(), []
        for d, s in ranked:
            key = (d.metadata.get("source_id") or d.metadata.get("id") or d.metadata.get("name"),
                   d.metadata.get("chunk_idx"))
            if key in seen:
                continue
            seen.add(key)
            ranked_docs.append(d)
            if len(ranked_docs) >= final_k:
                break
    return ranked_docs

# --- System prompt ---
SYSTEM_PROMPT_TMPL = """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.

Y√äU C·∫¶U:
- Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß √Ω ch√≠nh.
- Ghi r√µ Ngu·ªìn (trong ng·ªØ li·ªáu) v√† tr√≠ch d·∫´n ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm/T√™n lu·∫≠t (n·∫øu xu·∫•t hi·ªán trong ng·ªØ li·ªáu).
- KH√îNG b·ªãa, KH√îNG suy ƒëo√°n ngo√†i d·ªØ li·ªáu.
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin: "T√¥i kh√¥ng c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ ch·∫Øc ch·∫Øn."

Context:
{context}
"""

def build_context(docs):
    blocks = []
    for doc in docs:
        name = doc.metadata.get("name") or doc.metadata.get("file_name") or doc.metadata.get("source_id") or "Kh√¥ng r√µ ngu·ªìn"
        blocks.append(f"[Ngu·ªìn: {name}]\n{doc.page_content}")
    return "\n\n".join(blocks)

# ==== Mic button (floating & visible above chat_input) ====
prompt = st.chat_input("H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ Lu·∫≠t H√¥n Nh√¢n v√† Gia ƒê√¨nh")
with st.container():
  st.markdown('<div class="mic-fab">', unsafe_allow_html=True)
  audio_dict = mic_recorder(
      key="main_mic",
      start_prompt="üéôÔ∏è B·∫Øt ƒë·∫ßu ghi √¢m",
      stop_prompt="‚èπÔ∏è D·ª´ng ghi √¢m",
      just_once=False,   # b·∫•m ƒë·ªÉ ghi, b·∫•m l·∫ßn n·ªØa ƒë·ªÉ d·ª´ng
      format="wav"       # gi·ªØ 'wav' ƒë·ªÉ ƒë·ªçc b·∫±ng soundfile
  )


# N·∫øu v·ª´a d·ª´ng ghi √¢m ‚Üí nh·∫≠n d·∫°ng (ƒë·ªçc WAV t·ª´ bytes, t·ª± resample) ‚Üí ƒë·ªï prompt n·∫øu √¥ chat tr·ªëng
voice_text = None
if audio_dict and "bytes" in audio_dict and audio_dict["bytes"] and asr_pipe:
    with st.spinner("ƒêang nh·∫≠n d·∫°ng gi·ªçng n√≥i‚Ä¶"):
        audio_bytes = audio_dict["bytes"]
        # ƒê·ªçc WAV t·ª´ bytes (kh√¥ng c·∫ßn ffmpeg)
        data, sr = sf.read(io.BytesIO(audio_bytes), dtype="float32")
        if data.ndim > 1:  # stereo -> mono
            data = np.mean(data, axis=1)

        # L·∫•y sample rate mong mu·ªën c·ªßa model (th∆∞·ªùng 16000)
        try:
            target_sr = asr_pipe.feature_extractor.sampling_rate
        except Exception:
            target_sr = 16000

        # N·∫øu kh√°c SR, t·ª± resample b·∫±ng polyphase (kh·ªèi c·∫ßn torchaudio)
        if sr != target_sr:
            g = gcd(int(sr), int(target_sr))
            up = int(target_sr) // g
            down = int(sr) // g
            data = resample_poly(data, up, down).astype("float32")
            sr = target_sr

        input_audio = {"array": data, "sampling_rate": sr}

        # G·ªçi ASR (√©p ti·∫øng Vi·ªát, kh√¥ng d·ªãch)
        result = asr_pipe(input_audio, generate_kwargs={"language": "vi", "task": "transcribe"})
        voice_text = (result.get("text", "") or "").strip()
        if voice_text and not prompt:
            prompt = voice_text
            st.toast("üéôÔ∏è ƒê√£ l·∫•y vƒÉn b·∫£n t·ª´ gi·ªçng n√≥i", icon="‚úÖ")


# --- Run ---
if prompt:
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append(HumanMessage(prompt))

    # L·∫•y t√†i li·ªáu t·ªët nh·∫•t theo pipeline 3 t·∫ßng + (rerank n·∫øu c√≥)
    docs = retrieve_docs(prompt, final_k=6)

    # N·∫øu v·∫´n tr·ªëng, tr·∫£ l·ªùi an to√†n
    if not docs:
        safe_reply = "T√¥i kh√¥ng c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ ch·∫Øc ch·∫Øn."
        with st.chat_message("assistant"):
            st.markdown(safe_reply)
        st.session_state.messages.append(AIMessage(safe_reply))
    else:
        docs_text = build_context(docs)
        system_prompt = SYSTEM_PROMPT_TMPL.format(context=docs_text)

        # G·ªçi Gemma ƒë·ªÉ tr·∫£ l·ªùi
        final_prompt = system_prompt + "\n\nQuestion: " + prompt + "\nAnswer:"
        inputs = tokenizer(final_prompt, return_tensors="pt", truncation=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        output = model.generate(
            **inputs,
            max_new_tokens=600,
            do_sample=True,
            temperature=0.7
        )
        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        result = decoded.split("Answer:")[-1].strip()

        with st.chat_message("assistant"):
            st.markdown(result)
        st.session_state.messages.append(AIMessage(result))

        # Hi·ªÉn th·ªã ngu·ªìn
        with st.expander("Ngu·ªìn tham kh·∫£o ƒë√£ d√πng"):
            for i, d in enumerate(docs, 1):
                name = (
                    d.metadata.get("name")
                    or d.metadata.get("file_name")
                    or d.metadata.get("source_id")
                    or "Kh√¥ng r√µ ngu·ªìn"
                )
                st.markdown(f"**{i}. {name}**")

# Nh·∫Øc n·∫øu ASR ch∆∞a s·∫µn s√†ng
if not asr_pipe:
    st.caption("‚ö†Ô∏è ASR ch∆∞a s·∫µn s√†ng (PhoWhisper/Whisper). Ki·ªÉm tra HUGGINGFACE_HUB_TOKEN v√† k·∫øt n·ªëi m·∫°ng.")
